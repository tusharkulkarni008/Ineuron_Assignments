{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6bd03c",
   "metadata": {},
   "source": [
    "# Assignment 03 Solutions"
   ]
  },
    {
   "cell_type": "markdown",
   "id": "4a248256",
   "metadata": {},
   "source": [
    "#### 1. Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d1ac3d",
   "metadata": {},
   "source": [
    "No, it is not OK to initialize all the weights to the same value, even if that value is selected randomly using He initialization. Initializing all the weights to the same value would result in symmetric neurons, which would lead to symmetric gradients during backpropagation. As a result, all the neurons in the network would end up learning the same features, making the network redundant and reducing its learning capacity. Therefore, it is essential to initialize the weights with random values to break the symmetry and allow each neuron to learn unique features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5f184",
   "metadata": {},
   "source": [
    "#### 2. Is it OK to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785c03a6",
   "metadata": {},
   "source": [
    "Yes, it is generally OK to initialize the bias terms to 0. The bias terms provide a constant offset to the neurons, allowing them to shift the activation function's output. Initializing the biases to 0 initially ensures that the network starts with a neutral bias. During the training process, the biases will be adjusted based on the gradients and the data to learn appropriate values. However, if there are cases where initializing the biases to a specific value is known to be beneficial for the specific problem or architecture, it can be done accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7aaa4",
   "metadata": {},
   "source": [
    "#### 3. Name three advantages of the SELU activation function over ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975680c4",
   "metadata": {},
   "source": [
    "The three advantages of the SELU (Scaled Exponential Linear Unit) activation function over ReLU (Rectified Linear Unit) are:\n",
    "- SELU is a self-normalizing activation function, which helps address the vanishing/exploding gradients problem in deep neural networks. With SELU, the mean and variance of the inputs are maintained close to 0 and 1, respectively, allowing information to flow effectively through the network.\n",
    "- SELU preserves the mean and variance of the activations across layers, making it particularly suitable for deep neural networks. This property helps in training deep architectures more effectively.\n",
    "- SELU allows for the automatic adjustment of the weights during training. It introduces a form of self-normalization that can improve convergence and reduce the need for additional normalization techniques, such as Batch Normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cb7cc",
   "metadata": {},
   "source": [
    "#### 4. In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d645f3",
   "metadata": {},
   "source": [
    "Here are the use cases for the mentioned activation functions:\n",
    "- SELU: SELU is particularly useful in deep neural networks where the self-normalizing property helps address vanishing/exploding gradients. It is recommended to use SELU in architectures that can satisfy the necessary conditions for self-normalization (e.g., sequential dense layers, input data standardization, LeCun normal initialization).\n",
    "- Leaky ReLU (and its variants): Leaky ReLU is a good alternative to address the dying ReLU problem, where some neurons can become inactive and stop learning. It allows a small positive slope for negative inputs, preventing them from being completely deactivated. Leaky ReLU and its variants (e.g., Parametric ReLU, Randomized Leaky ReLU) can be used when ReLU is leading to dead neurons or when a small gradient for negative inputs is desired.\n",
    "- ReLU: ReLU is a widely used activation function that provides better learning capacity and faster convergence compared to activation functions like logistic or tanh. It is suitable for most deep learning tasks and is often used as the default choice for hidden layers in deep neural networks.\n",
    "- tanh: The hyperbolic tangent (tanh) activation function is useful in cases where the output range needs to be normalized between -1 and 1. It can be used in hidden layers of neural networks for classification or regression tasks. However, it is generally recommended to use ReLU or its variants instead of tanh for most scenarios.\n",
    "- logistic (sigmoid): The logistic activation function, also known as the sigmoid function, is commonly used in binary classification tasks. It squashes the input values between 0 and 1, representing the probability of the positive class. However, for most other tasks, ReLU or its variants are preferred due to their better learning properties and avoidance of vanishing gradients.\n",
    "- softmax: The softmax activation function is typically used in the output layer of a neural network for multiclass classification problems. It calculates the probabilities of each class and ensures they sum up to 1. Softmax is especially useful when dealing with mutually exclusive classes, and the goal is to assign the highest probability to the correct class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332cbd41",
   "metadata": {},
   "source": [
    "#### 5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59317265",
   "metadata": {},
   "source": [
    "Setting the momentum hyperparameter too close to 1 (e.g., 0.99999) in an SGD optimizer can lead to overshooting and unstable training. The momentum term in SGD determines the influence of previous gradients on the current update. When the momentum value is close to 1, the effect of previous gradients is amplified, and the optimizer tends to keep moving in the same direction, even when it should be slowing down or changing direction based on the current gradient.\n",
    "\n",
    "As a result, the optimizer can overshoot the optimal solution, causing oscillations or instability in the training process. The high momentum value prevents the optimizer from effectively exploring the search space and converging to a stable solution. It may also lead to the model getting stuck in suboptimal regions or diverging completely, resulting in poor performance or failure to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e479268",
   "metadata": {},
   "source": [
    "#### 6. Name three ways you can produce a sparse model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0446a69c",
   "metadata": {},
   "source": [
    "Three ways to produce a sparse model are as follows:\n",
    "- L1 Regularization (Lasso): By adding an L1 penalty term to the loss function, the weights of the model are pushed towards zero, leading to many weights becoming exactly zero. This encourages sparsity in the model as it selects only the most important features and eliminates the less relevant ones.\n",
    "- Dropout: Dropout is a regularization technique where randomly selected neurons are ignored or 'dropped out' during training. By dropping out neurons, the model becomes less reliant on specific neurons and encourages the remaining neurons to learn more robust representations. This can lead to sparse activation patterns in the network.\n",
    "- Pruning: Pruning involves removing or setting small weights to zero after the model has been trained. This can be done based on a threshold value or using techniques like magnitude-based pruning or structured pruning. Pruning eliminates connections or entire neurons with negligible contributions, resulting in a sparse model with reduced parameters and computational requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71bd58a",
   "metadata": {},
   "source": [
    "#### 7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0528d9",
   "metadata": {},
   "source": [
    "Dropout can slightly slow down training since it randomly drops out neurons during each training step, requiring additional computations. However, it can help regularize the model and prevent overfitting, which can improve the overall training time by reducing the need for other forms of regularization or excessive hyperparameter tuning.\n",
    "\n",
    "During inference or making predictions on new instances, dropout does not slow down the process. Inference only involves using the trained model to make predictions, and dropout is not applied during this phase.\n",
    "\n",
    "MC Dropout (Monte Carlo Dropout), on the other hand, involves performing multiple forward passes with dropout during inference. It can slow down the inference process compared to regular inference without dropout since multiple passes are made to obtain predictions. However, MC Dropout provides uncertainty estimates along with predictions, which can be useful in certain applications like Bayesian neural networks or active learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d16db75",
   "metadata": {},
   "source": [
    "#### 8.\tPractice training a deep neural network on the CIFAR10 image dataset:\n",
    "1.\tBuild a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.\n",
    "2.\tUsing Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.\n",
    "3.\tNow try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?\n",
    "4.\tTry replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).\n",
    "5.\tTry regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c256c5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
