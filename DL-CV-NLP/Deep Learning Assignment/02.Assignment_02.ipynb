{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Describe the structure of an artificial neuron. How is it similar to a biological neuron? What are its main components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An artificial neuron, also known as a perceptron, is a fundamental building block of an artificial neural network. It is designed to simulate the behavior of a biological neuron and process information in a similar way. The structure of an artificial neuron consists of the following main components:\n",
    "\n",
    "- Inputs: An artificial neuron receives inputs from other neurons or external sources. Each input is associated with a weight, which determines the importance or contribution of that input to the neuron's output.\n",
    "\n",
    "- Weights: The weights in an artificial neuron represent the strength or significance of each input. They act as adjustable parameters that the neuron learns during the training process. The weights are multiplied by the corresponding inputs and summed up to produce a weighted sum.\n",
    "\n",
    "- Bias: The bias is an additional input to the neuron that allows it to adjust the output based on a certain threshold or activation level. It can be thought of as the neuron's tendency to fire or activate.\n",
    "\n",
    "- Activation Function: The activation function determines the output of the neuron based on the weighted sum of inputs and the bias. It introduces non-linearity to the neuron, allowing it to learn and represent complex patterns and relationships in the data.\n",
    "\n",
    "- Output: The output of an artificial neuron is the result of applying the activation function to the weighted sum of inputs and the bias. It can represent a binary value (e.g., 0 or 1) or a continuous value depending on the problem being solved.\n",
    "\n",
    "Similar to a biological neuron, an artificial neuron receives inputs, processes them using weighted connections, and produces an output based on an activation level. The weights in an artificial neuron correspond to the synaptic strengths in a biological neuron, and the activation function mimics the firing behavior of a biological neuron.\n",
    "\n",
    "Overall, the structure of an artificial neuron allows it to model and learn complex relationships in data, making it a powerful tool for various machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the different types of activation functions popularly used? Explain each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions play a crucial role in artificial neural networks by introducing non-linearity and enabling the modeling of complex relationships between inputs and outputs. Some commonly used activation functions include:\n",
    "\n",
    "- **Sigmoid Function**: The sigmoid function, also known as the logistic function, is a smooth S-shaped curve. It maps the weighted sum of inputs and bias to a value between 0 and 1. The sigmoid function is expressed as:\n",
    "\n",
    "   $$f(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "   The sigmoid function is popularly used in binary classification problems where the output represents the probability of belonging to a certain class.\n",
    "\n",
    "- **Hyperbolic Tangent (Tanh) Function**: The hyperbolic tangent function is similar to the sigmoid function but maps the weighted sum of

 inputs and bias to a value between -1 and 1. It is expressed as:\n",
    "\n",
    "   $$f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "   The tanh function is commonly used in recurrent neural networks and hidden layers of neural networks.\n",
    "\n",
    "- **Rectified Linear Unit (ReLU)**: The ReLU function computes the output as the maximum of zero and the weighted sum of inputs and bias. Mathematically, it is defined as:\n",
    "\n",
    "   $$f(x) = \\max(0, x)$$\n",
    "\n",
    "   ReLU is widely used in deep learning models as it helps alleviate the vanishing gradient problem and speeds up the convergence of neural networks.\n",
    "\n",
    "- **Leaky ReLU**: The leaky ReLU is a variant of the ReLU function that allows a small negative slope for negative inputs. It is defined as:\n",
    "\n",
    "   $$f(x) = \\max(0.01x, x)$$\n",
    "\n",
    "   The leaky ReLU addresses the dying ReLU problem where neurons can become stuck in a state of zero output for negative inputs.\n",
    "\n",
    "- **Softmax Function**: The softmax function is used in the output layer of a neural network for multi-class classification problems. It normalizes the outputs such that they represent the probabilities of each class. The softmax function is defined as:\n",
    "\n",
    "   $$f(x_i) = \\frac{e^{x_i}}{\\sum_{j}e^{x_j}}$$\n",
    "\n",
    "   The softmax function ensures that the sum of the probabilities for all classes is equal to 1.\n",
    "\n",
    "These activation functions provide non-linear transformations that allow neural networks to model complex relationships and make them more powerful and expressive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Explain the below statements ?\n",
    "1. Explain, in detail, Rosenblatt’s perceptron model. How can a set of data be classified using a simple perceptron?\n",
    "2. Use a simple perceptron with weights w0, w1, and w2 as -1, 2, and 1, respectively, to classify data points (3, 4); (5, 2); (1, -3); (-8, -3); (-3, 0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statement 1:** Explain, in detail, Rosenblatt’s perceptron model. How can a set of data be classified using a simple perceptron?\n",
    "\n",
    "Rosenblatt's perceptron model is one of the earliest and simplest forms of artificial neural networks. It consists of a single artificial neuron (perceptron) that can make binary classifications. The model is based on the concept of threshold activation, where the neuron produces a specific output if the weighted sum of inputs exceeds a certain threshold. The steps involved in classifying data using a simple perceptron are as follows:\n",
    "\n",
    "1. Initialize the weights and bias of the perceptron randomly or with some predefined values.\n",
    "\n",
    "2. For each input in

 the dataset, compute the weighted sum of inputs and bias:\n",
    "\n",
    "   $$weighted\\_sum = w_0 + w_1x_1 + w_2x_2 + \\ldots + w_nx_n$$\n",
    "\n",
    "   where $w_0, w_1, w_2, \\ldots, w_n$ are the weights and $x_1, x_2, \\ldots, x_n$ are the input features.\n",
    "\n",
    "3. Apply an activation function to the weighted sum to determine the output of the perceptron. Commonly used activation functions include step function, sigmoid function, or ReLU.\n",
    "\n",
    "4. Compare the output of the perceptron with the desired output or the ground truth label. If the output matches the desired class, proceed to the next input. If the output differs, adjust the weights and bias to minimize the error.\n",
    "\n",
    "5. Repeat steps 2-4 for a certain number of iterations or until the perceptron achieves the desired accuracy.\n",
    "\n",
    "The perceptron model uses the concept of **supervised learning** where the correct class labels are known for the training dataset. By adjusting the weights and bias based on the error, the perceptron gradually learns to classify the input data correctly. The learning process is driven by an iterative optimization algorithm, which updates the weights and bias to minimize the error between the perceptron's output and the expected output.\n",
    "\n",
    "Once the perceptron is trained, it can be used to classify new data points by applying the learned weights and bias to the input features. The output of the perceptron will indicate the predicted class or category of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statement 2:** Use a simple perceptron with weights w0, w1, and w2 as -1, 2, and 1, respectively, to classify data points (3, 4); (5, 2); (1, -3); (-8, -3); (-3, 0).\n",
    "\n",
    "To classify the given data points using a simple perceptron with weights w0 = -1, w1 = 2, and w2 = 1, we need to compute the weighted sum of inputs and bias for each data point and apply the activation function to determine the predicted class. Let's calculate the predictions:\n",
    "\n",
    "Data Point (3, 4):\n",
    "Weighted sum = (-1) + (2)(3) + (1)(4) = -1 + 6 + 4 = 9\n",
    "Using a step function as the activation function, the output will be:\n",
    "Output = step(9) = 1 (assuming the step function returns 1 for values greater than or equal to 0)\n",
    "\n",
    "Data Point (5, 2):\n",
    "Weighted sum = (-1) + (2)(5) + (1)(2) = -1 + 10 + 2 = 11\n",
    "Output = step(11) = 1\n",
    "\n",
    "Data Point (1, -3):\n",
    "Weighted sum = (-1)

 + (2)(1) + (1)(-3) = -1 + 2 - 3 = -2\n",
    "Output = step(-2) = 0 (assuming the step function returns 0 for values less than 0)\n",
    "\n",
    "Data Point (-8, -3):\n",
    "Weighted sum = (-1) + (2)(-8) + (1)(-3) = -1 - 16 - 3 = -20\n",
    "Output = step(-20) = 0\n",
    "\n",
    "Data Point (-3, 0):\n",
    "Weighted sum = (-1) + (2)(-3) + (1)(0) = -1 - 6 + 0 = -7\n",
    "Output = step(-7) = 0\n",
    "\n",
    "Based on the calculations, the predicted classes for the given data points using the simple perceptron are as follows:\n",
    "(3, 4) -> Class 1\n",
    "(5, 2) -> Class 1\n",
    "(1, -3) -> Class 0\n",
    "(-8, -3) -> Class 0\n",
    "(-3, 0) -> Class 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
